{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2153a1a6-a9c3-4cb1-92d0-6690cdb34f54",
   "metadata": {},
   "source": [
    "# Obtain Sentiments for a Single Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6383967-df22-4786-b5f6-cbcfade2de45",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c946f28-06e9-4354-b187-cade75c70073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a37a097c744dac9ae1111db9e64963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks Processed:   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Progress bar lib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Dataset paths\n",
    "#\n",
    "# If paths do not have any datasets, run huggingface_hub.hf_hub_download \n",
    "# with repo_id=\"Zihan1004/FNSPID\", filename=\"Stock_news/nasdaq_exteral_data.csv, and repo_type=\"dataset\"\n",
    "# A similar process can be used for the All_external.csv dataset\n",
    "# Both these datasets have not been fully scraped\n",
    "nasdaq_path = \"../../news_data/direct_data/nasdaq_external_data.csv\"\n",
    "\n",
    "# Stock ticker\n",
    "tick = \"NVDA\"\n",
    "\n",
    "# List of dataframes\n",
    "nasdaq_df = []\n",
    "\n",
    "# Chunk calculation\n",
    "CHUNK_SIZE = 10000\n",
    "ROWS = 2476902\n",
    "NCHUNKS = ROWS // CHUNK_SIZE\n",
    "\n",
    "# Chunk progress\n",
    "chunk_number = 1\n",
    "\n",
    "# Boolean tracking whether currently on ticker\n",
    "# hit_tick = False\n",
    "# passed_tick = False\n",
    "\n",
    "# Progress bar\n",
    "bar = tqdm(total=NCHUNKS, desc=\"Chunks Processed\")\n",
    "\n",
    "# ROWS may be adjusted if scraping occurs or original data is changed\n",
    "\n",
    "# Read csv in chunks for lower memory usage\n",
    "for chunk in pd.read_csv(\n",
    "    nasdaq_path,\n",
    "    usecols=[\"Date\", \"Stock_symbol\", \"Lsa_summary\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"],\n",
    "    dtype=np.bytes_,\n",
    "    chunksize=CHUNK_SIZE,\n",
    "    # skiprows=lambda x: passed_tick\n",
    "):\n",
    "\n",
    "    # Start index at 0\n",
    "    chunk = chunk.reset_index(drop=True)\n",
    "    \n",
    "    # Ticker column index\n",
    "    tick_col = chunk.columns.get_loc(\"Stock_symbol\")\n",
    "\n",
    "    # Add function0--maybe less efficient but does a full scan:\n",
    "    for row in range(chunk.shape[0]):\n",
    "        if chunk.iloc[row, tick_col] == tick:\n",
    "            nasdaq_df.append(chunk[chunk[\"Stock_symbol\"] == tick])\n",
    "            break\n",
    "    # Add chunks of ticker news\n",
    "    # if hit_tick and not passed_tick:\n",
    "    #     passed_tick = chunk.iloc[chunk.shape[0] - 1, tick_col] != tick\n",
    "    #     nasdaq_df.append(chunk) if not passed_tick else nasdaq_df.append(chunk[\"Stock_symbol\" == tick])\n",
    "    # else if not passed_tick:\n",
    "    #     for row in range(chunk.shape[0]):\n",
    "    #         if chunk.iloc[row, tick_col] == tick:\n",
    "    #             nasdaq_df.append(chunk[chunk[\"Stock_symbol\"] == tick])\n",
    "    #             break\n",
    "\n",
    "    bar.update(1)\n",
    "\n",
    "# Make the dataframe\n",
    "nasdaq_df = pd.concat(nasdaq_df)\n",
    "nasdaq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5258a-24ef-4ff7-81d2-b87c61e50a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for redundant data\n",
    "nasdaq_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Initial amount of data\n",
    "initial = nasdaq_df.size\n",
    "print(f\"Initial number of elements: {initial}\")\n",
    "\n",
    "# Check for missing data\n",
    "nasdaq_df.dropna(inplace=True)\n",
    "print(f\"Post drop number of elements: {nasdaq_df.size}\")\n",
    "\n",
    "# Print data loss\n",
    "print(f\"Overall number of elements dropped (including missing elements): {initial - nasdaq_df.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da2245-362f-48e7-9874-f67db256b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ðŸ¤— pipeline\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Setting up ðŸ¤— pipeline\n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    device=0,\n",
    ")\n",
    "\n",
    "# Index tracking variables\n",
    "lsa, luhn, textrank, lexrank = 0, 0, 0, 0\n",
    "\n",
    "# Data functions for pipeline to score using\n",
    "def nasdaq_lsa():\n",
    "    for i in range(nasdaq_df.shape[0]):\n",
    "        yield nasdaq_df.iloc[i][\"Lsa_summary\"]\n",
    "\n",
    "def nasdaq_luhn():\n",
    "    for i in range(nasdaq_df.shape[0]):\n",
    "        yield nasdaq_df.iloc[i][\"Luhn_summary\"]\n",
    "\n",
    "def nasdaq_text():\n",
    "    for i in range(nasdaq_df.shape[0]):\n",
    "        yield nasdaq_df.iloc[i][\"Textrank_summary\"]\n",
    "\n",
    "def nasdaq_lex():\n",
    "    for i in range(nasdaq_df.shape[0]):\n",
    "        yield nasdaq_df.iloc[i][\"Lexrank_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de090227-08a1-41dc-9525-ddf83b4f656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Columns added\n",
    "for bot in [\"Lsa\", \"Luhn\", \"Textrank\", \"Lexrank\"]:\n",
    "    nasdaq_df[bot + \"_sentiment\"] = \"\"\n",
    "    nasdaq_df[bot + \"_confidence\"] = \"\"\n",
    "\n",
    "# Indexes of columns\n",
    "lsa_sloc = nasdaq_df.columns.get_loc(\"Lsa_sentiment\")\n",
    "lsa_cloc = nasdaq_df.columns.get_loc(\"Lsa_confidence\")\n",
    "luhn_sloc = nasdaq_df.columns.get_loc(\"Luhn_sentiment\")\n",
    "luhn_cloc = nasdaq_df.columns.get_loc(\"Luhn_confidence\")\n",
    "textrank_sloc = nasdaq_df.columns.get_loc(\"Textrank_sentiment\")\n",
    "textrank_cloc = nasdaq_df.columns.get_loc(\"Textrank_confidence\")\n",
    "lexrank_sloc = nasdaq_df.columns.get_loc(\"Lexrank_sentiment\")\n",
    "lexrank_cloc = nasdaq_df.columns.get_loc(\"Lexrank_confidence\")\n",
    "\n",
    "# Lsa sentiment\n",
    "for sentiment in tqdm(finbert(nasdaq_lsa()), total=nasdaq_df.shape[0], desc=\"LSA Sentiment using FinBERT\"):\n",
    "    nasdaq_df.iloc[lsa, lsa_sloc] = sentiment[\"label\"]\n",
    "    nasdaq_df.iloc[lsa, lsa_cloc] = sentiment[\"score\"]\n",
    "    lsa += 1\n",
    "\n",
    "# Luhn sentiment\n",
    "for sentiment in tqdm(finbert(nasdaq_lsa()), total=nasdaq_df.shape[0], desc=\"Luhn Sentiment using FinBERT\"):\n",
    "    nasdaq_df.iloc[luhn, luhn_sloc] = sentiment[\"label\"]\n",
    "    nasdaq_df.iloc[luhn, luhn_cloc] = sentiment[\"score\"]\n",
    "    luhn += 1\n",
    "\n",
    "# Textrank sentiment\n",
    "for sentiment in tqdm(finbert(nasdaq_lsa()), total=nasdaq_df.shape[0], desc=\"TextRank Sentiment using FinBERT\"):\n",
    "    nasdaq_df.iloc[textrank, textrank_sloc] = sentiment[\"label\"]\n",
    "    nasdaq_df.iloc[textrank, textrank_cloc] = sentiment[\"score\"]\n",
    "    textrank += 1\n",
    "\n",
    "# Lexrank sentiment\n",
    "for sentiment in tqdm(finbert(nasdaq_lsa()), total=nasdaq_df.shape[0], desc=\"LexRank Sentiment using FinBERT\"):\n",
    "    nasdaq_df.iloc[lexrank, lexrank_sloc] = sentiment[\"label\"]\n",
    "    nasdaq_df.iloc[lexrank, lexrank_cloc] = sentiment[\"score\"]\n",
    "    lexrank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7dc3a-76fa-495c-ade6-0d535238b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop summaries now that sentiments have been assigned\n",
    "nasdaq_df.drop(columns=[\"Lsa_summary\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"], inplace=True)\n",
    "nasdaq_df.reset_index(drop=True, inplace=True)\n",
    "nasdaq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181259d3-1ad8-44fc-afef-12c61f1e5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv\n",
    "os.makedirs(f\"../{tick.lower()}_data\", exist_ok=True)\n",
    "nasdaq_df.to_csv(f\"../{tick.lower()}_data/sentiments.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Exported to csv at \" + f\"single_stock/{tick.lower()}_data/sentiments.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (stockbot-env)",
   "language": "python",
   "name": "stockbot-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
